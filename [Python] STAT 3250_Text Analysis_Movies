##  This assignment requires data from four files: 
##
##      'movies.txt':  A file of over 3900 movies
##      'users.dat':   A file of over 6000 reviewers who provided ratings
##      'ratings.dat': A file of over 1,000,000 movie ratings
##      'zips.txt':    A file of zip codes and location information
##
##  The file 'readme.txt' has more information about the first three files.
##  You will need to consult the readme file to answer some of the questions.

import numpy as np # load numpy as np
import pandas as pd # load pandas as pd

##  Note: You will need to convert the zip code information in 'users.dat' into
##  state (or territory) information for one or more of the questions below.
##  You must use the information in 'zips.txt' for this purpose, you cannot
##  use other conversion methods. 

# Read in data
movies = open('/Users/mac/Desktop/UVA/2018-2019/STAT 3250/Assignments/movies.txt').read().splitlines()
users = open('/Users/mac/Desktop/UVA/2018-2019/STAT 3250/Assignments/users.dat').read().splitlines()
ratings = open('/Users/mac/Desktop/UVA/2018-2019/STAT 3250/Assignments/ratings.dat').read().splitlines()
zips = open('/Users/mac/Desktop/UVA/2018-2019/STAT 3250/Assignments/zipcodes.txt').read().splitlines()

# Make dataframes
df1 = pd.DataFrame()
df1['MovieID'] = pd.Series(movies).str.split('::').str[0].astype(int)
df1['Title'] = pd.Series(movies).str.split('::').str[1]
df1['Genre'] = pd.Series(movies).str.split('::').str[2]

df2 = pd.DataFrame()
df2['UserID'] = pd.Series(users).str.split('::').str[0].astype(int)
df2['Gender'] = pd.Series(users).str.split('::').str[1]
df2['Age'] = pd.Series(users).str.split('::').str[2].astype(int)
df2['Occupation'] = pd.Series(users).str.split('::').str[3].astype(int)
df2['Zip-code'] = pd.Series(users).str.split('::').str[4]

df3 = pd.DataFrame()
df3['UserID'] = pd.Series(ratings).str.split('::').str[0].astype(int)
df3['MovieID'] = pd.Series(ratings).str.split('::').str[1].astype(int)
df3['Rating'] = pd.Series(ratings).str.split('::').str[2].astype(int)
df3['Timestamp'] = pd.Series(ratings).str.split('::').str[3]

df4 = pd.DataFrame()
df4 = pd.read_csv('/Users/mac/Desktop/UVA/2018-2019/STAT 3250/zipcodes.txt',
                  usecols = [1,4],
                  converters={'Zipcode':str})
df4

df4 = df4.drop_duplicates()

df4.columns = ('Zip-code', 'State')

# Dictionary for occupation title
occupationtitle = dict={
    0:  "other or not specified",
	1:  "academic/educator",
	2:  "artist",
	3:  "clerical/admin",
	4:  "college/grad student",
	5:  "customer service",
	6:  "doctor/health care",
	7:  "executive/managerial",
	8:  "farmer",
	9:  "homemaker",
	10:  "K-12 student",
	11:  "lawyer",
	12:  "programmer",
	13:  "retired",
	14:  "sales/marketing",
	15:  "scientist",
	16:  "self-employed",
	17:  "technician/engineer",
	18:  "tradesman/craftsman",
	19:  "unemployed",
	20:  "writer" }

## 1.  Determine the percentage of users that are female.  Do the same for the
##     percentage of users in the 35-44 age group.  In the 18-24 age group,
##     determine the percentage of male users.

# Female percentage
np.sum(df2['Gender']=='F') / len(df2) * 100
# Female percentage 35-44
np.sum((df2['Age'] == 35) & (df2['Gender']=='F')) / np.sum(df2['Age'] == 35) * 100
# Male percentage 18-24
np.sum((df2['Age'] == 18) & (df2['Gender']=='M')) / np.sum(df2['Age'] == 35) * 100
"""
28.295%
28.332%
67.477%
"""

## 2.  Give a year-by-year table of counts for the number of ratings, sorted by
##     year in ascending order.

# Read timestamp with to_datetime
df3['Timestamp'] = pd.to_datetime(df3['Timestamp'], unit='s')
# extract the year and count
df3['Rating'].groupby([df3['Timestamp'].dt.year]).count().sort_values()
"""
2003      3348
2002     24046
2001     68058
2000    904757
"""          

## 3.  Determine the average rating for females and the average rating for 
##     males.

# Merge rating with gender
data = pd.merge(df2, df3)
# groupby gender and get the mean
data['Rating'].groupby(data['Gender']).mean()

"""
Gender
F    3.620366
M    3.568879
"""

## 4.  Find the top-10 movies based on average rating.  (Movies and remakes 
##     should be considered different.)  Give a table with the movie title
##     (including the year) and the average rating, sorted by rating from
##     highest to lowest.  (Include ties as needed.)

# merge rating with title
data = pd.merge(data, df1, on='MovieID')

# Top 10
data['Rating'].groupby(data['Title']).mean().sort_values(ascending=False)[0:19]
"""

Title
Gate of Heavenly Peace, The (1995)                                     5.000000
Lured (1947)                                                           5.000000
Ulysses (Ulisse) (1954)                                                5.000000
Smashing Time (1967)                                                   5.000000
Follow the Bitch (1998)                                                5.000000
Song of Freedom (1936)                                                 5.000000
Bittersweet Motel (2000)                                               5.000000
Baby, The (1973)                                                       5.000000
One Little Indian (1973)                                               5.000000
Schlafes Bruder (Brother of Sleep) (1995)                              5.000000
I Am Cuba (Soy Cuba/Ya Kuba) (1964)                                    4.800000
Lamerica (1994)                                                        4.750000
Apple, The (Sib) (1998)                                                4.666667
Sanjuro (1962)                                                         4.608696
Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)    4.560510
Shawshank Redemption, The (1994)                                       4.554558
Godfather, The (1972)                                                  4.524966
Close Shave, A (1995)                                                  4.520548
Usual Suspects, The (1995)                                             4.517106
"""

## 5.  Determine the number of movies listed in 'movies.txt' for which there
##     is no rating.  Determine the percentage of these unrated movies for
##     which there is a more recent remake.

# Movies with no rating
norating = df1[(~df1['MovieID'].isin(np.unique(df3['MovieID'])))]

# Number of movies with no rating
len(norating)
# Percentage of movies with remakes
(len(norating) - len(norating.groupby(norating['Title'].str[:-6]).count())) / len(norating) * 100
"""
177
0
"""

## 6.  Determine the average rating for each occupation classification 
##     (including 'other or not specified'), and give the results in a
##     table sorted from highest to lowest average and including the
##     occupation title.

# Merge Occupatino and Rating
df6 = pd.merge(df2[['UserID', 'Occupation']], df3[['UserID','Rating']])

# Map Occupation Title
df6['Occupation Title']=(df6['Occupation'].astype(float)).map(occupationtitle) 

# Get average rating and sort
df6['Rating'].groupby(df6['Occupation Title']).mean().sort_values(ascending=False)

"""
Occupation Title
retired                   3.781736
scientist                 3.689774
doctor/health care        3.661578
homemaker                 3.656589
clerical/admin            3.656516
programmer                3.654001
sales/marketing           3.618481
lawyer                    3.617371
technician/engineer       3.613574
executive/managerial      3.599772
self-employed             3.596575
academic/educator         3.576642
artist                    3.573081
other or not specified    3.537544
customer service          3.537529
college/grad student      3.536793
K-12 student              3.532675
tradesman/craftsman       3.530117
writer                    3.497392
farmer                    3.466741
unemployed                3.414050
"""

## 7.  Determine the average rating for each genre, and give the results in
##     a table listing genre and average rating in descending order.

# Split the genres
eachgenre = np.unique(data['Genre'].str.split('|'))
# Aggregate in a big list
eachgenrelist = sum(eachgenre, [])
# List of genres
genrelist = np.unique(eachgenrelist)

# Get the mean rating for each genre
means = []
for genre in genrelist:
    subsets = data[data['Genre'].str.contains(genre)]
    mean = np.mean(subsets['Rating'])
    means.append(mean)

# Make a data frame
pd.DataFrame({'Genre':genrelist,
              'Average rating':means}).sort_values(by='Average rating',
    ascending=False)

"""
          Genre  Average rating
9     Film-Noir        4.075188
6   Documentary        3.933123
16          War        3.893327
7         Drama        3.766332
5         Crime        3.708679
2     Animation        3.684868
12      Mystery        3.668102
11      Musical        3.665519
17      Western        3.637770
13      Romance        3.607465
15     Thriller        3.570466
4        Comedy        3.522099
0        Action        3.491185
1     Adventure        3.477257
14       Sci-Fi        3.466521
8       Fantasy        3.447371
3    Children's        3.422035
10       Horror        3.215013
"""

## 8.  For the user age category, assume that the user has age at the midpoint
##     of the given range.  (For instance '35-44' has age (35+44)/2 = 39.5)
##     For 'under 18' assume an age of 16, and for '56+' assume an age of 60.
##     For each possible rating (1-5) determine the average age of the raters.

# Count of age groups
agecounts = pd.Series(list(data['Age'].groupby([data['Rating'], data['Age']]).count()))
# Median ages
medianages = [(1+18)/2, (18+24)/2, (25+34)/2, (35+44)/2,
              (45+49)/2, (50+55)/2, 60]

df8 = pd.DataFrame()
# Individual sums
df8['indsum'] = agecounts * pd.Series((medianages*5))

# List of median ages
df8['rating'] = pd.Series(sum([[1]*7,[2]*7,[3]*7,[4]*7,[5]*7],[]))

# Average age by rating group
(df8.groupby('rating').sum())['indsum'] / (data['Age'].groupby(data['Rating']).count())

"""
rating
1    31.451819
2    32.589213
3    33.681903
4    34.106850
5    34.172909
"""


## 9.  Find all combinations (if there are any) of occupation and genre for 
##     which there are no ratings.

# Counts of combinations
df9=data['Rating'].groupby([data['Occupation'],data['Genre']]).count().reset_index(name='count')
# Rows with 0 count
df9[df9['count']==0]

"""
Empty DataFrame
No combination
"""

## 10. For each age group, determine the occupation that gave the lowest 
##     average rating.  Give a table that includes the age group, occupation,
##     and average rating.  (Sort by age group from youngest to oldest)

# Map occupation title
data['Occupation Title']=(data['Occupation'].astype(float)).map(occupationtitle) 

# Group by age and occupation title and get the mean
df10 = data['Rating'].groupby([data['Age'],data['Occupation Title']]).mean().reset_index()

# Define column names
df10.columns = ['Age','Occupation Title','Rating']

# Subset out the rows minimum rating
df10[df10.groupby(['Age'])['Rating'].transform(min) == df10['Rating']]

"""
     Age      Occupation Title    Rating
6      1                lawyer  3.066667
19    18    doctor/health care  3.235525
51    25            unemployed  3.366426
61    35                farmer  2.642045
77    45  college/grad student  3.280000
101   50                farmer  3.437610
127   56       sales/marketing  3.291755
"""

## 11. Find the top-5 states in terms of average rating.  Give in table form
##     including the state and average rating, sorted from highest to lowest.
##     Note: If any of the zip codes in 'users.dat' includes letters, then we
##     classify that user as being from Canada, which we treat as a state for
##     this and the next question.

# Merge data with the zipcode
data = pd.merge(data, df4)

# Top 5 states
data['Rating'].groupby(data['State']).mean().sort_values(ascending=False)[0:5]

"""
State
GU    4.236842
MS    3.996409
AK    3.985730
AP    3.938967
SC    3.816553
"""

## 12. For each genre, determine which state produced the most reviews.  
##     (Include any ties.)

# Tie list
ties = []
# List of states with most reviews
maxstates = []

# Get the state with most reviews for each genre
# Check if there is a tie with the state with the second most reviews
for genre in genrelist:
    tiecheck = 0
    subsets = data[data['Genre'].str.contains(genre)]
    states = subsets['State']
    statecount = states.value_counts().sort_values(ascending=False).reset_index()
    statecount.columns = ('State', 'Count')
    maxstate = statecount.loc[0,'State']
    if statecount.loc[0,'Count'] == statecount.loc[1,'Count']:
        tiecheck += 1
    maxstates.append(maxstate)
    ties.append(tiecheck)

print(ties)
"""
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
There is no tie
"""

# Dataframe
pd.DataFrame({'Genre':genrelist,
              'State':maxstates})

"""
          Genre State
0        Action    CA
1     Adventure    CA
2     Animation    CA
3    Children's    CA
4        Comedy    CA
5         Crime    CA
6   Documentary    CA
7         Drama    CA
8       Fantasy    CA
9     Film-Noir    CA
10       Horror    CA
11      Musical    CA
12      Mystery    CA
13      Romance    CA
14       Sci-Fi    CA
15     Thriller    CA
16          War    CA
17      Western    CA
"""
